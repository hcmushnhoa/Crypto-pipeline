{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T14:56:54.438097Z",
     "start_time": "2025-12-04T14:56:44.380718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "#from pandas.tests.frame.methods.test_sort_values import ascending\n",
    "from datetime import datetime, timezone, timedelta\n",
    "os.environ[\"SPARK_HOME\"] = \"/mnt/d/learn/DE/Semina_project/spark\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "import boto3\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,StringType,ArrayType\n",
    "from pyspark.sql.functions import input_file_name, explode, col,lit, date_format, to_timestamp\n",
    "import duckdb\n",
    "import pyarrow as pa"
   ],
   "id": "7948c80c40a3d59c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T15:02:49.204073Z",
     "start_time": "2025-12-04T15:02:49.175655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "S3_ENDPOINT = \"http://localhost:9000\"\n",
    "S3_ACCESS = \"minio\"\n",
    "S3_SECRET = \"minio123\"\n",
    "S3_BUCKET = \"trading-okx\"\n",
    "def get_latest_file(bucket_name, prefix,days_lookback):\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        endpoint_url=S3_ENDPOINT,\n",
    "        aws_access_key_id=S3_ACCESS,\n",
    "        aws_secret_access_key=S3_SECRET,\n",
    "        region_name=\"us-east-1\"\n",
    "    )\n",
    "    all_objects = []\n",
    "    # Paginator is used to list if more 1000 files\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n",
    "    for page in page_iterator:\n",
    "        for obj in page.get('Contents', []):\n",
    "            key=obj['Key']\n",
    "            last_modified = obj['LastModified']\n",
    "            if last_modified > datetime.now(timezone.utc)- timedelta(days=days_lookback):\n",
    "                 all_objects.append(obj)\n",
    "    all_objects.sort(key=lambda x: x['LastModified'], reverse=True)\n",
    "    #latest_files = all_objects[:limit]\n",
    "    #latest_files = all_objects\n",
    "    paths = [f\"s3a://{bucket_name}/{obj['Key']}\" for obj in all_objects]\n",
    "    return paths\n",
    "\n",
    "latest_files = get_latest_file(\n",
    "        bucket_name=S3_BUCKET,\n",
    "        prefix=\"bronze/okx_ohlc\",\n",
    "        days_lookback=1 # thay đổi khi run lại toàn bộ\n",
    ")\n",
    "latest_files"
   ],
   "id": "f6794b21713da3c4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s3a://trading-okx/bronze/okx_ohlc/2025-12-04/21/okx_ohlc_20251204_214557.jsonl.gz',\n",
       " 's3a://trading-okx/bronze/okx_ohlc/2025-12-04/21/okx_ohlc_20251204_214553.jsonl.gz',\n",
       " 's3a://trading-okx/bronze/okx_ohlc/2025-12-04/21/okx_ohlc_20251204_214554.jsonl.gz',\n",
       " 's3a://trading-okx/bronze/okx_ohlc/2025-12-04/21/okx_ohlc_20251204_214552.jsonl.gz']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T14:57:33.948434Z",
     "start_time": "2025-12-04T14:57:04.193682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OKX_Bronze_To_Silver_Book_new\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", S3_ENDPOINT) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", S3_ACCESS) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", S3_SECRET) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .getOrCreate()"
   ],
   "id": "da62bae69543c77b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n",
      "bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n",
      "25/12/04 21:57:25 WARN Utils: Your hostname, hoalaptop resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/12/04 21:57:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/mnt/d/learn/DE/Semina_project/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/hoalaptop/.ivy2/cache\n",
      "The jars for the packages stored in: /home/hoalaptop/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0fffab50-0c84-4e89-bfea-052f4a3d9585;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 325ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0fffab50-0c84-4e89-bfea-052f4a3d9585\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/11ms)\n",
      "25/12/04 21:57:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T15:02:57.524108Z",
     "start_time": "2025-12-04T15:02:57.266927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "candle_array_schema = ArrayType(StringType())\n",
    "schema = StructType([\n",
    "    StructField(\"received_at\", StringType(), True),\n",
    "    StructField(\"payload\", StructType([\n",
    "        # Lấy thêm thông tin channel để biết instId (Symbol)\n",
    "        StructField(\"arg\", StructType([\n",
    "            StructField(\"instId\", StringType(), True),\n",
    "            StructField(\"channel\", StringType(), True)\n",
    "        ]), True),\n",
    "        # Data là mảng của các mảng\n",
    "        StructField(\"data\", ArrayType(candle_array_schema), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "df = spark.read.schema(schema).json(latest_files)\n",
    "df.show(truncate=False)"
   ],
   "id": "da20810c52270ec4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "|received_at               |payload                                                                                                            |\n",
      "+--------------------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "|2025-12-04T21:36:03.390932|{{BTC-USDT-SWAP, candle1s}, [[1764858960000, 92570.9, 92597.9, 92560, 92580.3, 2165.9, 21.659, 2005248.35297, 1]]} |\n",
      "|2025-12-04T21:36:08.509532|{{BTC-USDT-SWAP, candle1s}, [[1764858967000, 92550.7, 92552.9, 92542.9, 92542.9, 68.79, 0.6879, 63664.15765, 1]]}  |\n",
      "|2025-12-04T21:36:25.655413|{{BTC-USDT-SWAP, candle1s}, [[1764859400000, 92504.1, 92504.1, 92504.1, 92504.1, 9.53, 0.0953, 8815.64073, 1]]}    |\n",
      "|2025-12-04T21:43:33.237312|{{BTC-USDT-SWAP, candle1s}, [[1764859412000, 92585.8, 92593.1, 92585.8, 92593.1, 216.54, 2.1654, 200490.25483, 1]]}|\n",
      "|2025-12-04T21:21:01.200220|{{BTC-USDT-SWAP, candle1s}, [[1764858058000, 92580, 92580, 92580, 92580, 3.68, 0.0368, 3406.944, 1]]}              |\n",
      "|2025-12-04T21:36:20.712961|{{BTC-USDT-SWAP, candle1s}, [[1764859252000, 92672, 92672, 92655.1, 92655.1, 319.84, 3.1984, 296370.2405, 1]]}     |\n",
      "|2025-12-04T21:36:20.712971|{{BTC-USDT-SWAP, candle1s}, [[1764859253000, 92655, 92655.1, 92655, 92655.1, 135.14, 1.3514, 125214.07403, 1]]}    |\n",
      "|2025-12-04T21:36:20.712979|{{BTC-USDT-SWAP, candle1s}, [[1764859254000, 92655, 92655.1, 92647, 92654, 379.15, 3.7915, 351296.2818, 1]]}       |\n",
      "|2025-12-04T21:36:20.712986|{{BTC-USDT-SWAP, candle1s}, [[1764859255000, 92654.9, 92672, 92654.9, 92672, 31.11, 0.3111, 28826.62057, 1]]}      |\n",
      "|2025-12-04T21:36:20.712993|{{BTC-USDT-SWAP, candle1s}, [[1764859256000, 92671.9, 92672, 92665.1, 92665.1, 9.27, 0.0927, 8590.25604, 1]]}      |\n",
      "|2025-12-04T21:20:53.720420|{{BTC-USDT-SWAP, candle1s}, [[1764858051000, 92577.5, 92577.5, 92577.4, 92577.4, 3.18, 0.0318, 2943.96376, 1]]}    |\n",
      "|2025-12-04T21:20:55.780136|{{BTC-USDT-SWAP, candle1s}, [[1764858053000, 92577.4, 92577.4, 92577.4, 92577.4, 0, 0, 0, 1]]}                     |\n",
      "|2025-12-04T21:21:07.618554|{{BTC-USDT-SWAP, candle1s}, [[1764858064000, 92597.9, 92597.9, 92597.9, 92597.9, 0.03, 0.0003, 27.77937, 1]]}      |\n",
      "|2025-12-04T21:21:11.625948|{{BTC-USDT-SWAP, candle1s}, [[1764858070000, 92597.9, 92598, 92597.9, 92598, 8, 0.08, 7407.83999, 1]]}             |\n",
      "|2025-12-04T21:21:13.813729|{{BTC-USDT-SWAP, candle1s}, [[1764858072000, 92608.2, 92608.2, 92608.1, 92608.1, 6.36, 0.0636, 5889.87541, 1]]}    |\n",
      "|2025-12-04T21:36:22.319881|{{BTC-USDT-SWAP, candle1s}, [[1764859397000, 92542.8, 92545, 92533.3, 92533.3, 69.74, 0.6974, 64540.06991, 1]]}    |\n",
      "|2025-12-04T21:43:23.154782|{{BTC-USDT-SWAP, candle1s}, [[1764859402000, 92509.2, 92509.2, 92509.2, 92509.2, 30.05, 0.3005, 27799.0146, 1]]}   |\n",
      "+--------------------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T15:03:52.046321Z",
     "start_time": "2025-12-04T15:03:51.583017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_exploded = df.select(\n",
    "    col(\"received_at\"),\n",
    "    col(\"payload.arg.instId\").alias(\"symbol\"),\n",
    "    col(\"payload.arg.channel\").alias(\"channel\"),  # Ví dụ: candle1s, candle1m\n",
    "    explode(col(\"payload.data\")).alias(\"candle\")  # Mỗi dòng là 1 mảng [\"ts\", \"o\", ...]\n",
    ")\n",
    "df_silver = df_exploded.select(\n",
    "    col(\"symbol\"),\n",
    "    col(\"channel\"),\n",
    "    to_timestamp(col(\"candle\")[0].cast(\"long\") / 1000).alias(\"candle_time\"),\n",
    "    col(\"candle\")[1].cast(\"double\").alias(\"open\"),\n",
    "    col(\"candle\")[2].cast(\"double\").alias(\"high\"),\n",
    "    col(\"candle\")[3].cast(\"double\").alias(\"low\"),\n",
    "    col(\"candle\")[4].cast(\"double\").alias(\"close\"),\n",
    "    col(\"candle\")[5].cast(\"double\").alias(\"volume\"),\n",
    "    col(\"candle\")[8].cast(\"int\").alias(\"is_confirmed\"),\n",
    "    col(\"received_at\").cast(\"timestamp\").alias(\"ingestion_time\")\n",
    ")\n",
    "df_silver.show()"
   ],
   "id": "71e3d6a447b36d4d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+-------------------+-------+-------+-------+-------+------+------------+--------------------+\n",
      "|       symbol| channel|        candle_time|   open|   high|    low|  close|volume|is_confirmed|      ingestion_time|\n",
      "+-------------+--------+-------------------+-------+-------+-------+-------+------+------------+--------------------+\n",
      "|BTC-USDT-SWAP|candle1s|2025-12-04 21:36:00|92570.9|92597.9|92560.0|92580.3|2165.9|           1|2025-12-04 21:36:...|\n",
      "|BTC-USDT-SWAP|candle1s|2025-12-04 21:36:07|92550.7|92552.9|92542.9|92542.9| 68.79|           1|2025-12-04 21:36:...|\n",
      "|BTC-USDT-SWAP|candle1s|2025-12-04 21:43:20|92504.1|92504.1|92504.1|92504.1|  9.53|           1|2025-12-04 21:36:...|\n",
      "|BTC-USDT-SWAP|candle1s|2025-12-04 21:43:32|92585.8|92593.1|92585.8|92593.1|216.54|           1|2025-12-04 21:43:...|\n",
      "|BTC-USDT-SWAP|candle1s|2025-12-04 21:20:58|92580.0|92580.0|92580.0|92580.0|  3.68|           1|2025-12-04 21:21:...|\n",
      "|BTC-USDT-SWAP|candle1s|2025-12-04 21:40:52|92672.0|92672.0|92655.1|92655.1|319.84|           1|2025-12-04 21:36:...|\n",
      "|BTC-USDT-SWAP|candle1s|2025-12-04 21:40:53|92655.0|92655.1|92655.0|92655.1|135.14|           1|2025-12-04 21:36:...|\n",
      "|BTC-USDT-SWAP|candle1s|2025-12-04 21:40:54|92655.0|92655.1|92647.0|92654.0|379.15|           1|2025-12-04 21:36:...|\n",
      "|BTC-USDT-SWAP|candle1s|2025-12-04 21:40:55|92654.9|92672.0|92654.9|92672.0| 31.11|           1|2025-12-04 21:36:...|\n",
      "|BTC-USDT-SWAP|candle1s|2025-12-04 21:40:56|92671.9|92672.0|92665.1|92665.1|  9.27|           1|2025-12-04 21:36:...|\n",
      "|BTC-USDT-SWAP|candle1s|2025-12-04 21:20:51|92577.5|92577.5|92577.4|92577.4|  3.18|           1|2025-12-04 21:20:...|\n",
      "|BTC-USDT-SWAP|candle1s|2025-12-04 21:20:53|92577.4|92577.4|92577.4|92577.4|   0.0|           1|2025-12-04 21:20:...|\n",
      "|BTC-USDT-SWAP|candle1s|2025-12-04 21:21:04|92597.9|92597.9|92597.9|92597.9|  0.03|           1|2025-12-04 21:21:...|\n",
      "|BTC-USDT-SWAP|candle1s|2025-12-04 21:21:10|92597.9|92598.0|92597.9|92598.0|   8.0|           1|2025-12-04 21:21:...|\n",
      "|BTC-USDT-SWAP|candle1s|2025-12-04 21:21:12|92608.2|92608.2|92608.1|92608.1|  6.36|           1|2025-12-04 21:21:...|\n",
      "|BTC-USDT-SWAP|candle1s|2025-12-04 21:43:17|92542.8|92545.0|92533.3|92533.3| 69.74|           1|2025-12-04 21:36:...|\n",
      "|BTC-USDT-SWAP|candle1s|2025-12-04 21:43:22|92509.2|92509.2|92509.2|92509.2| 30.05|           1|2025-12-04 21:43:...|\n",
      "+-------------+--------+-------------------+-------+-------+-------+-------+------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T15:04:22.481893Z",
     "start_time": "2025-12-04T15:04:22.207900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#df.show(truncate=False)\n",
    "\n",
    "#df_exploded.show(truncate=False)\n",
    "# B3.2: Ép kiểu và Chọn cột\n",
    "\n",
    "#df_silver.show(truncate=False)\n",
    "df_cleaned = df_silver \\\n",
    "    .dropna(subset=[\"open\", \"candle_time\"]) \\\n",
    "    .withColumn(\"date_part\", date_format(col(\"candle_time\"), \"yyyy-MM-dd\"))\n",
    "df_cleaned.show()\n"
   ],
   "id": "740652257d91478a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+-------------------+-------+-------+-------+-------+------+------------+--------------------+----------+\n",
      "|       symbol| channel|        candle_time|   open|   high|    low|  close|volume|is_confirmed|      ingestion_time| date_part|\n",
      "+-------------+--------+-------------------+-------+-------+-------+-------+------+------------+--------------------+----------+\n",
      "|BTC-USDT-SWAP|candle1s|2025-12-04 21:36:00|92570.9|92597.9|92560.0|92580.3|2165.9|           1|2025-12-04 21:36:...|2025-12-04|\n",
      "|BTC-USDT-SWAP|candle1s|2025-12-04 21:36:07|92550.7|92552.9|92542.9|92542.9| 68.79|           1|2025-12-04 21:36:...|2025-12-04|\n",
      "|BTC-USDT-SWAP|candle1s|2025-12-04 21:43:20|92504.1|92504.1|92504.1|92504.1|  9.53|           1|2025-12-04 21:36:...|2025-12-04|\n",
      "|BTC-USDT-SWAP|candle1s|2025-12-04 21:43:32|92585.8|92593.1|92585.8|92593.1|216.54|           1|2025-12-04 21:43:...|2025-12-04|\n",
      "|BTC-USDT-SWAP|candle1s|2025-12-04 21:20:58|92580.0|92580.0|92580.0|92580.0|  3.68|           1|2025-12-04 21:21:...|2025-12-04|\n",
      "|BTC-USDT-SWAP|candle1s|2025-12-04 21:40:52|92672.0|92672.0|92655.1|92655.1|319.84|           1|2025-12-04 21:36:...|2025-12-04|\n",
      "|BTC-USDT-SWAP|candle1s|2025-12-04 21:40:53|92655.0|92655.1|92655.0|92655.1|135.14|           1|2025-12-04 21:36:...|2025-12-04|\n",
      "|BTC-USDT-SWAP|candle1s|2025-12-04 21:40:54|92655.0|92655.1|92647.0|92654.0|379.15|           1|2025-12-04 21:36:...|2025-12-04|\n",
      "|BTC-USDT-SWAP|candle1s|2025-12-04 21:40:55|92654.9|92672.0|92654.9|92672.0| 31.11|           1|2025-12-04 21:36:...|2025-12-04|\n",
      "|BTC-USDT-SWAP|candle1s|2025-12-04 21:40:56|92671.9|92672.0|92665.1|92665.1|  9.27|           1|2025-12-04 21:36:...|2025-12-04|\n",
      "|BTC-USDT-SWAP|candle1s|2025-12-04 21:20:51|92577.5|92577.5|92577.4|92577.4|  3.18|           1|2025-12-04 21:20:...|2025-12-04|\n",
      "|BTC-USDT-SWAP|candle1s|2025-12-04 21:20:53|92577.4|92577.4|92577.4|92577.4|   0.0|           1|2025-12-04 21:20:...|2025-12-04|\n",
      "|BTC-USDT-SWAP|candle1s|2025-12-04 21:21:04|92597.9|92597.9|92597.9|92597.9|  0.03|           1|2025-12-04 21:21:...|2025-12-04|\n",
      "|BTC-USDT-SWAP|candle1s|2025-12-04 21:21:10|92597.9|92598.0|92597.9|92598.0|   8.0|           1|2025-12-04 21:21:...|2025-12-04|\n",
      "|BTC-USDT-SWAP|candle1s|2025-12-04 21:21:12|92608.2|92608.2|92608.1|92608.1|  6.36|           1|2025-12-04 21:21:...|2025-12-04|\n",
      "|BTC-USDT-SWAP|candle1s|2025-12-04 21:43:17|92542.8|92545.0|92533.3|92533.3| 69.74|           1|2025-12-04 21:36:...|2025-12-04|\n",
      "|BTC-USDT-SWAP|candle1s|2025-12-04 21:43:22|92509.2|92509.2|92509.2|92509.2| 30.05|           1|2025-12-04 21:43:...|2025-12-04|\n",
      "+-------------+--------+-------------------+-------+-------+-------+-------+------+------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T15:05:24.030680Z",
     "start_time": "2025-12-04T15:05:24.024921Z"
    }
   },
   "cell_type": "code",
   "source": "df_cleaned.printSchema()",
   "id": "7f100dd0980c07ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- symbol: string (nullable = true)\n",
      " |-- channel: string (nullable = true)\n",
      " |-- candle_time: timestamp (nullable = true)\n",
      " |-- open: double (nullable = true)\n",
      " |-- high: double (nullable = true)\n",
      " |-- low: double (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      " |-- volume: double (nullable = true)\n",
      " |-- is_confirmed: integer (nullable = true)\n",
      " |-- ingestion_time: timestamp (nullable = true)\n",
      " |-- date_part: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "output = f\"s3a://trading-okx/silver/okx-funding/\"\n",
    "df_silver.write.mode('append').format(\"parquet\").save(output)"
   ],
   "id": "363e3b1d56088f38",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# config dw duckdb",
   "id": "4a5505312eb9b69b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T15:21:14.760299Z",
     "start_time": "2025-12-04T15:21:14.736983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# connect to duckdb\n",
    "duck_path='/mnt/d/learn/DE/Semina_project/datawarehouse.duckdb'\n",
    "con=duckdb.connect(duck_path)"
   ],
   "id": "2476a25c19f0c7be",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T15:21:20.315768Z",
     "start_time": "2025-12-04T15:21:20.308619Z"
    }
   },
   "cell_type": "code",
   "source": "con.close()",
   "id": "a3eae653ba3e4b71",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T15:16:11.284640Z",
     "start_time": "2025-12-04T15:16:11.263105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# scrip create table\n",
    "with open('/mnt/d/learn/DE/Semina_project/SQL_db/config_database/source_db.sql', 'r') as f:\n",
    "    sql_script = f.read()\n",
    "con.execute(sql_script)"
   ],
   "id": "ab0804d8c4a4dc6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_duckdb.DuckDBPyConnection at 0x7f40cfeac2b0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T15:21:17.309622Z",
     "start_time": "2025-12-04T15:21:17.295629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# test table\n",
    "con.sql(\"show TABLEs ; \")\n",
    "#con.sql(\"select * from fact_ohlc;\")\n"
   ],
   "id": "5b3121194beca17c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────────────────┐\n",
       "│       name        │\n",
       "│      varchar      │\n",
       "├───────────────────┤\n",
       "│ fact_funding_rate │\n",
       "│ fact_mark_price   │\n",
       "│ fact_ohlc         │\n",
       "│ fact_order_books  │\n",
       "│ fact_trades       │\n",
       "└───────────────────┘"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# read file sql to connect to minio and create dim fact table\n",
    "with open('/mnt/d/learn/DE/Semina_project/SQL_db/config_dw/warehouse_source.sql', 'r') as f:\n",
    "    sql_script = f.read()\n",
    "con.execute(sql_script)"
   ],
   "id": "fd9acd1b65198b18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T15:17:43.550527Z",
     "start_time": "2025-12-04T15:17:43.323171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "arrow_table = pa.Table.from_pandas(df_cleaned.toPandas())\n",
    "con.register(\"arrow_table\", arrow_table)\n",
    "con.sql('select * from arrow_table')\n",
    "#con.close()"
   ],
   "id": "3593a6823e8bcdd3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────────────┬──────────┬─────────────────────┬─────────┬─────────┬─────────┬─────────┬────────┬──────────────┬────────────────────────────┬────────────┐\n",
       "│    symbol     │ channel  │     candle_time     │  open   │  high   │   low   │  close  │ volume │ is_confirmed │       ingestion_time       │ date_part  │\n",
       "│    varchar    │ varchar  │    timestamp_ns     │ double  │ double  │ double  │ double  │ double │    int32     │        timestamp_ns        │  varchar   │\n",
       "├───────────────┼──────────┼─────────────────────┼─────────┼─────────┼─────────┼─────────┼────────┼──────────────┼────────────────────────────┼────────────┤\n",
       "│ BTC-USDT-SWAP │ candle1s │ 2025-12-04 21:36:00 │ 92570.9 │ 92597.9 │ 92560.0 │ 92580.3 │ 2165.9 │            1 │ 2025-12-04 21:36:03.390932 │ 2025-12-04 │\n",
       "│ BTC-USDT-SWAP │ candle1s │ 2025-12-04 21:36:07 │ 92550.7 │ 92552.9 │ 92542.9 │ 92542.9 │  68.79 │            1 │ 2025-12-04 21:36:08.509532 │ 2025-12-04 │\n",
       "│ BTC-USDT-SWAP │ candle1s │ 2025-12-04 21:43:20 │ 92504.1 │ 92504.1 │ 92504.1 │ 92504.1 │   9.53 │            1 │ 2025-12-04 21:36:25.655413 │ 2025-12-04 │\n",
       "│ BTC-USDT-SWAP │ candle1s │ 2025-12-04 21:43:32 │ 92585.8 │ 92593.1 │ 92585.8 │ 92593.1 │ 216.54 │            1 │ 2025-12-04 21:43:33.237312 │ 2025-12-04 │\n",
       "│ BTC-USDT-SWAP │ candle1s │ 2025-12-04 21:20:58 │ 92580.0 │ 92580.0 │ 92580.0 │ 92580.0 │   3.68 │            1 │ 2025-12-04 21:21:01.20022  │ 2025-12-04 │\n",
       "│ BTC-USDT-SWAP │ candle1s │ 2025-12-04 21:40:52 │ 92672.0 │ 92672.0 │ 92655.1 │ 92655.1 │ 319.84 │            1 │ 2025-12-04 21:36:20.712961 │ 2025-12-04 │\n",
       "│ BTC-USDT-SWAP │ candle1s │ 2025-12-04 21:40:53 │ 92655.0 │ 92655.1 │ 92655.0 │ 92655.1 │ 135.14 │            1 │ 2025-12-04 21:36:20.712971 │ 2025-12-04 │\n",
       "│ BTC-USDT-SWAP │ candle1s │ 2025-12-04 21:40:54 │ 92655.0 │ 92655.1 │ 92647.0 │ 92654.0 │ 379.15 │            1 │ 2025-12-04 21:36:20.712979 │ 2025-12-04 │\n",
       "│ BTC-USDT-SWAP │ candle1s │ 2025-12-04 21:40:55 │ 92654.9 │ 92672.0 │ 92654.9 │ 92672.0 │  31.11 │            1 │ 2025-12-04 21:36:20.712986 │ 2025-12-04 │\n",
       "│ BTC-USDT-SWAP │ candle1s │ 2025-12-04 21:40:56 │ 92671.9 │ 92672.0 │ 92665.1 │ 92665.1 │   9.27 │            1 │ 2025-12-04 21:36:20.712993 │ 2025-12-04 │\n",
       "│ BTC-USDT-SWAP │ candle1s │ 2025-12-04 21:20:51 │ 92577.5 │ 92577.5 │ 92577.4 │ 92577.4 │   3.18 │            1 │ 2025-12-04 21:20:53.72042  │ 2025-12-04 │\n",
       "│ BTC-USDT-SWAP │ candle1s │ 2025-12-04 21:20:53 │ 92577.4 │ 92577.4 │ 92577.4 │ 92577.4 │    0.0 │            1 │ 2025-12-04 21:20:55.780136 │ 2025-12-04 │\n",
       "│ BTC-USDT-SWAP │ candle1s │ 2025-12-04 21:21:04 │ 92597.9 │ 92597.9 │ 92597.9 │ 92597.9 │   0.03 │            1 │ 2025-12-04 21:21:07.618554 │ 2025-12-04 │\n",
       "│ BTC-USDT-SWAP │ candle1s │ 2025-12-04 21:21:10 │ 92597.9 │ 92598.0 │ 92597.9 │ 92598.0 │    8.0 │            1 │ 2025-12-04 21:21:11.625948 │ 2025-12-04 │\n",
       "│ BTC-USDT-SWAP │ candle1s │ 2025-12-04 21:21:12 │ 92608.2 │ 92608.2 │ 92608.1 │ 92608.1 │   6.36 │            1 │ 2025-12-04 21:21:13.813729 │ 2025-12-04 │\n",
       "│ BTC-USDT-SWAP │ candle1s │ 2025-12-04 21:43:17 │ 92542.8 │ 92545.0 │ 92533.3 │ 92533.3 │  69.74 │            1 │ 2025-12-04 21:36:22.319881 │ 2025-12-04 │\n",
       "│ BTC-USDT-SWAP │ candle1s │ 2025-12-04 21:43:22 │ 92509.2 │ 92509.2 │ 92509.2 │ 92509.2 │  30.05 │            1 │ 2025-12-04 21:43:23.154782 │ 2025-12-04 │\n",
       "├───────────────┴──────────┴─────────────────────┴─────────┴─────────┴─────────┴─────────┴────────┴──────────────┴────────────────────────────┴────────────┤\n",
       "│ 17 rows                                                                                                                                       11 columns │\n",
       "└──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T15:17:54.768176Z",
     "start_time": "2025-12-04T15:17:54.725212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "con.execute('''\n",
    "                INSERT INTO fact_ohlc(\n",
    "                                            symbol,\n",
    "                                            channel,\n",
    "                                            candle_time,\n",
    "                                            open,\n",
    "                                            high,\n",
    "                                            low,\n",
    "                                            close,\n",
    "                                            volume,\n",
    "                                            is_confirmed,\n",
    "                                            ingestion_time,\n",
    "                                            date_part)\n",
    "                SELECT\n",
    "                    symbol ,\n",
    "                    channel ,\n",
    "                    candle_time ,\n",
    "                    open ,\n",
    "                    high ,\n",
    "                    low ,\n",
    "                    close ,\n",
    "                    volume ,\n",
    "                    is_confirmed ,\n",
    "                    ingestion_time ,\n",
    "                    date_part\n",
    "                FROM arrow_table\n",
    "                ''')"
   ],
   "id": "192ad9190d9f3b84",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_duckdb.DuckDBPyConnection at 0x7f40cfeac2b0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# read file sql to connect to minio and create dim fact table\n",
    "with open('/mnt/d/learn/DE/Semina_project/SQL_db/config_dw/warehouse_source.sql', 'r') as f:\n",
    "    sql_script = f.read()\n",
    "con.execute(sql_script)\n",
    "\n",
    "output_path = f\"s3://trading-okx/silver/okx-funding/*.parquet\"\n",
    "try:\n",
    "    # Chỉ load dữ liệu mới (Ví dụ lọc theo ngày nếu cần)\n",
    "    # Ở đây load hết (Full Load)\n",
    "    con.execute(f\"\"\"\n",
    "        INSERT INTO fact_funding_rate\n",
    "        SELECT * FROM read_parquet('{output_path}')\n",
    "       --WHERE ingestion_time NOT IN (SELECT ingestion_time FROM fact_orders_books) -- Tránh trùng lặp (Dedup đơn giản)\n",
    "    \"\"\")\n",
    "\n",
    "    # Lấy số dòng đã insert\n",
    "    count = con.execute(\"SELECT count(*) FROM fact_funding_rate\").fetchone()[0]\n",
    "    print(f\"✅ Data loaded. Total rows in DuckDB: {count}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading data: {e}\")\n"
   ],
   "id": "2aca4480477753e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "con.close()\n",
    "spark.stop()"
   ],
   "id": "9ac9823548815d76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T15:22:58.120443Z",
     "start_time": "2025-12-03T15:22:58.113054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import boto3\n",
    "import pyarrow as pa\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,StringType,ArrayType\n",
    "from pyspark.sql.functions import input_file_name, explode, col, date_format\n",
    "import duckdb\n",
    "''' code dùng cho fact table'''\n",
    "S3_ENDPOINT = \"http://localhost:9000\"\n",
    "S3_ACCESS = \"minio\"\n",
    "S3_SECRET = \"minio123\"\n",
    "S3_BUCKET = \"trading-okx\"\n",
    "#\n",
    "def get_latest_file(bucket_name, prefix,days_lookback):\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        endpoint_url=S3_ENDPOINT,\n",
    "        aws_access_key_id=S3_ACCESS,\n",
    "        aws_secret_access_key=S3_SECRET,\n",
    "        region_name=\"us-east-1\"\n",
    "    )\n",
    "    all_objects = []\n",
    "    # Paginator is used to list if more 1000 files\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n",
    "    for page in page_iterator:\n",
    "        for obj in page.get('Contents', []):\n",
    "            key=obj['Key']\n",
    "            last_modified = obj['LastModified']\n",
    "            if last_modified > datetime.now(timezone.utc)- timedelta(days=days_lookback):\n",
    "                 all_objects.append(obj)\n",
    "    all_objects.sort(key=lambda x: x['LastModified'], reverse=True)\n",
    "    #latest_files = all_objects[:limit]\n",
    "    latest_files = all_objects\n",
    "    paths = [f\"s3a://{bucket_name}/{obj['Key']}\" for obj in latest_files]\n",
    "    return paths\n"
   ],
   "id": "4a21a941d0e755ac",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T15:23:01.727436Z",
     "start_time": "2025-12-03T15:23:01.706700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "latest_files = get_latest_file(\n",
    "        bucket_name=\"trading-okx\",\n",
    "        prefix=\"bronze/okx_trades/\",\n",
    "        days_lookback=6\n",
    "    )\n",
    "latest_files"
   ],
   "id": "bf563064d4cf8320",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s3a://trading-okx/bronze/okx_trades/2025-12-03/14/okx_trades_20251203_143941.jsonl.gz',\n",
       " 's3a://trading-okx/bronze/okx_trades/2025-12-03/14/okx_trades_20251203_143932.jsonl.gz',\n",
       " 's3a://trading-okx/bronze/okx_trades/2025-12-03/14/okx_trades_20251203_143931.jsonl.gz',\n",
       " 's3a://trading-okx/bronze/okx_trades/2025-12-03/14/okx_trades_20251203_143930.jsonl.gz',\n",
       " 's3a://trading-okx/bronze/okx_trades/2025-12-03/14/okx_trades_20251203_143929.jsonl.gz',\n",
       " 's3a://trading-okx/bronze/okx_trades/2025-12-03/14/okx_trades_20251203_143928.jsonl.gz']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T15:23:19.257228Z",
     "start_time": "2025-12-03T15:23:19.243563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"OKX_Bronze_To_Silver\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", S3_ENDPOINT) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", S3_ACCESS) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", S3_SECRET) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .getOrCreate()\n"
   ],
   "id": "35c9c4cd773cc15b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/03 22:23:19 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T15:26:56.694249Z",
     "start_time": "2025-12-03T15:26:56.192822Z"
    }
   },
   "cell_type": "code",
   "source": "spark.stop()",
   "id": "ecdc5875bc25b774",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T15:23:24.223341Z",
     "start_time": "2025-12-03T15:23:23.481710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trade_schema = StructType([\n",
    "    StructField(\"instId\", StringType(), True),\n",
    "    StructField(\"tradeId\", StringType(), True),\n",
    "    StructField(\"px\", StringType(), True),\n",
    "    StructField(\"sz\", StringType(), True),\n",
    "    StructField(\"side\", StringType(), True),\n",
    "    StructField(\"ts\", StringType(), True),\n",
    "    StructField(\"count\", StringType(), True),\n",
    "    StructField(\"source\", StringType(), True),\n",
    "    StructField(\"seqId\", StringType(), True)\n",
    "])\n",
    "schema = StructType([\n",
    "    StructField(\"received_at\", StringType(), True),\n",
    "    StructField(\"payload\", StructType([\n",
    "        StructField(\"data\", ArrayType(trade_schema), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "df = spark.read.schema(schema).json(latest_files)\n",
    "df.show()"
   ],
   "id": "c28377bcc935687d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/03 22:23:23 ERROR Executor: Exception in task 0.0 in stage 8.0 (TID 24)\n",
      "org.apache.spark.SparkException: Encountered error while reading file s3a://trading-okx/bronze/okx_trades/2025-12-03/14/okx_trades_20251203_143941.jsonl.gz. Details:\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.IOException: not a gzip file\n",
      "\tat org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.processBasicHeader(BuiltInGzipDecompressor.java:496)\n",
      "\tat org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.executeHeaderState(BuiltInGzipDecompressor.java:257)\n",
      "\tat org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.decompress(BuiltInGzipDecompressor.java:186)\n",
      "\tat org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:111)\n",
      "\tat org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:105)\n",
      "\tat java.base/java.io.InputStream.read(InputStream.java:205)\n",
      "\tat org.apache.hadoop.util.LineReader.fillBuffer(LineReader.java:191)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n",
      "\t... 22 more\n",
      "25/12/03 22:23:23 WARN TaskSetManager: Lost task 0.0 in stage 8.0 (TID 24) (10.255.255.254 executor driver): org.apache.spark.SparkException: Encountered error while reading file s3a://trading-okx/bronze/okx_trades/2025-12-03/14/okx_trades_20251203_143941.jsonl.gz. Details:\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.IOException: not a gzip file\n",
      "\tat org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.processBasicHeader(BuiltInGzipDecompressor.java:496)\n",
      "\tat org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.executeHeaderState(BuiltInGzipDecompressor.java:257)\n",
      "\tat org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.decompress(BuiltInGzipDecompressor.java:186)\n",
      "\tat org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:111)\n",
      "\tat org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:105)\n",
      "\tat java.base/java.io.InputStream.read(InputStream.java:205)\n",
      "\tat org.apache.hadoop.util.LineReader.fillBuffer(LineReader.java:191)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n",
      "\t... 22 more\n",
      "\n",
      "25/12/03 22:23:23 ERROR TaskSetManager: Task 0 in stage 8.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o390.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 24) (10.255.255.254 executor driver): org.apache.spark.SparkException: Encountered error while reading file s3a://trading-okx/bronze/okx_trades/2025-12-03/14/okx_trades_20251203_143941.jsonl.gz. Details:\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: not a gzip file\n\tat org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.processBasicHeader(BuiltInGzipDecompressor.java:496)\n\tat org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.executeHeaderState(BuiltInGzipDecompressor.java:257)\n\tat org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.decompress(BuiltInGzipDecompressor.java:186)\n\tat org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:111)\n\tat org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:105)\n\tat java.base/java.io.InputStream.read(InputStream.java:205)\n\tat org.apache.hadoop.util.LineReader.fillBuffer(LineReader.java:191)\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\t... 22 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Encountered error while reading file s3a://trading-okx/bronze/okx_trades/2025-12-03/14/okx_trades_20251203_143941.jsonl.gz. Details:\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: java.io.IOException: not a gzip file\n\tat org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.processBasicHeader(BuiltInGzipDecompressor.java:496)\n\tat org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.executeHeaderState(BuiltInGzipDecompressor.java:257)\n\tat org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.decompress(BuiltInGzipDecompressor.java:186)\n\tat org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:111)\n\tat org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:105)\n\tat java.base/java.io.InputStream.read(InputStream.java:205)\n\tat org.apache.hadoop.util.LineReader.fillBuffer(LineReader.java:191)\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\t... 22 more\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mPy4JJavaError\u001B[39m                             Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[17]\u001B[39m\u001B[32m, line 20\u001B[39m\n\u001B[32m     12\u001B[39m schema = StructType([\n\u001B[32m     13\u001B[39m     StructField(\u001B[33m\"\u001B[39m\u001B[33mreceived_at\u001B[39m\u001B[33m\"\u001B[39m, StringType(), \u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[32m     14\u001B[39m     StructField(\u001B[33m\"\u001B[39m\u001B[33mpayload\u001B[39m\u001B[33m\"\u001B[39m, StructType([\n\u001B[32m     15\u001B[39m         StructField(\u001B[33m\"\u001B[39m\u001B[33mdata\u001B[39m\u001B[33m\"\u001B[39m, ArrayType(trade_schema), \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m     16\u001B[39m     ]), \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m     17\u001B[39m ])\n\u001B[32m     19\u001B[39m df = spark.read.schema(schema).json(latest_files)\n\u001B[32m---> \u001B[39m\u001B[32m20\u001B[39m \u001B[43mdf\u001B[49m\u001B[43m.\u001B[49m\u001B[43mshow\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/mnt/d/learn/DE/Semina_project/venv_linux/lib/python3.12/site-packages/pyspark/sql/dataframe.py:945\u001B[39m, in \u001B[36mDataFrame.show\u001B[39m\u001B[34m(self, n, truncate, vertical)\u001B[39m\n\u001B[32m    885\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mshow\u001B[39m(\u001B[38;5;28mself\u001B[39m, n: \u001B[38;5;28mint\u001B[39m = \u001B[32m20\u001B[39m, truncate: Union[\u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mint\u001B[39m] = \u001B[38;5;28;01mTrue\u001B[39;00m, vertical: \u001B[38;5;28mbool\u001B[39m = \u001B[38;5;28;01mFalse\u001B[39;00m) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    886\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Prints the first ``n`` rows to the console.\u001B[39;00m\n\u001B[32m    887\u001B[39m \n\u001B[32m    888\u001B[39m \u001B[33;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    943\u001B[39m \u001B[33;03m    name | Bob\u001B[39;00m\n\u001B[32m    944\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m945\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_show_string\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtruncate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/mnt/d/learn/DE/Semina_project/venv_linux/lib/python3.12/site-packages/pyspark/sql/dataframe.py:963\u001B[39m, in \u001B[36mDataFrame._show_string\u001B[39m\u001B[34m(self, n, truncate, vertical)\u001B[39m\n\u001B[32m    957\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[32m    958\u001B[39m         error_class=\u001B[33m\"\u001B[39m\u001B[33mNOT_BOOL\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    959\u001B[39m         message_parameters={\u001B[33m\"\u001B[39m\u001B[33marg_name\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mvertical\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33marg_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(vertical).\u001B[34m__name__\u001B[39m},\n\u001B[32m    960\u001B[39m     )\n\u001B[32m    962\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(truncate, \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m truncate:\n\u001B[32m--> \u001B[39m\u001B[32m963\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_jdf\u001B[49m\u001B[43m.\u001B[49m\u001B[43mshowString\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    964\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    965\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/mnt/d/learn/DE/Semina_project/venv_linux/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001B[39m, in \u001B[36mJavaMember.__call__\u001B[39m\u001B[34m(self, *args)\u001B[39m\n\u001B[32m   1316\u001B[39m command = proto.CALL_COMMAND_NAME +\\\n\u001B[32m   1317\u001B[39m     \u001B[38;5;28mself\u001B[39m.command_header +\\\n\u001B[32m   1318\u001B[39m     args_command +\\\n\u001B[32m   1319\u001B[39m     proto.END_COMMAND_PART\n\u001B[32m   1321\u001B[39m answer = \u001B[38;5;28mself\u001B[39m.gateway_client.send_command(command)\n\u001B[32m-> \u001B[39m\u001B[32m1322\u001B[39m return_value = \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1323\u001B[39m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1325\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[32m   1326\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[33m\"\u001B[39m\u001B[33m_detach\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/mnt/d/learn/DE/Semina_project/venv_linux/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001B[39m, in \u001B[36mcapture_sql_exception.<locals>.deco\u001B[39m\u001B[34m(*a, **kw)\u001B[39m\n\u001B[32m    177\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdeco\u001B[39m(*a: Any, **kw: Any) -> Any:\n\u001B[32m    178\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m179\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    180\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    181\u001B[39m         converted = convert_exception(e.java_exception)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/mnt/d/learn/DE/Semina_project/venv_linux/lib/python3.12/site-packages/py4j/protocol.py:326\u001B[39m, in \u001B[36mget_return_value\u001B[39m\u001B[34m(answer, gateway_client, target_id, name)\u001B[39m\n\u001B[32m    324\u001B[39m value = OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[32m2\u001B[39m:], gateway_client)\n\u001B[32m    325\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[32m1\u001B[39m] == REFERENCE_TYPE:\n\u001B[32m--> \u001B[39m\u001B[32m326\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[32m    327\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m.\n\u001B[32m    328\u001B[39m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[33m\"\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m, name), value)\n\u001B[32m    329\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    330\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[32m    331\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[33m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m.\n\u001B[32m    332\u001B[39m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[33m\"\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m, name, value))\n",
      "\u001B[31mPy4JJavaError\u001B[39m: An error occurred while calling o390.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 24) (10.255.255.254 executor driver): org.apache.spark.SparkException: Encountered error while reading file s3a://trading-okx/bronze/okx_trades/2025-12-03/14/okx_trades_20251203_143941.jsonl.gz. Details:\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: not a gzip file\n\tat org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.processBasicHeader(BuiltInGzipDecompressor.java:496)\n\tat org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.executeHeaderState(BuiltInGzipDecompressor.java:257)\n\tat org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.decompress(BuiltInGzipDecompressor.java:186)\n\tat org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:111)\n\tat org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:105)\n\tat java.base/java.io.InputStream.read(InputStream.java:205)\n\tat org.apache.hadoop.util.LineReader.fillBuffer(LineReader.java:191)\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\t... 22 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Encountered error while reading file s3a://trading-okx/bronze/okx_trades/2025-12-03/14/okx_trades_20251203_143941.jsonl.gz. Details:\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: java.io.IOException: not a gzip file\n\tat org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.processBasicHeader(BuiltInGzipDecompressor.java:496)\n\tat org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.executeHeaderState(BuiltInGzipDecompressor.java:257)\n\tat org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.decompress(BuiltInGzipDecompressor.java:186)\n\tat org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:111)\n\tat org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:105)\n\tat java.base/java.io.InputStream.read(InputStream.java:205)\n\tat org.apache.hadoop.util.LineReader.fillBuffer(LineReader.java:191)\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\t... 22 more\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_exploded = df.select(\n",
    "    col(\"received_at\"),\n",
    "    explode(col(\"payload.data\")).alias(\"trade\")\n",
    ")\n",
    "df_silver = df_exploded.select(\n",
    "    col(\"trade.instId\").alias(\"symbol\"),\n",
    "    col(\"trade.tradeId\").alias(\"tradeId\"),\n",
    "    col(\"trade.side\").alias(\"side\"),\n",
    "    col(\"trade.px\").cast(\"double\").alias(\"price\"),\n",
    "    col(\"trade.sz\").cast(\"double\").alias(\"quantity\"),\n",
    "    (col(\"trade.ts\").cast(\"long\") / 1000).cast(\"timestamp\").alias(\"trade_time\"),\n",
    "    col(\"received_at\").cast(\"timestamp\").alias(\"ingestion_time\")\n",
    ")\n",
    "df_silver.show()"
   ],
   "id": "f328949b7b8bf65b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "df_cleaned = df_silver \\\n",
    "    .dropna(subset=[\"price\", \"quantity\"]) \\\n",
    "    .dropDuplicates([\"tradeId\"]) \\\n",
    "    .withColumn(\"date_part\", date_format(col(\"trade_time\"), \"yyyy-MM-dd\"))\n",
    "df_cleaned.show()\n"
   ],
   "id": "5a5009c7206a80d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# spark connect\n",
    "# should change config in .sh\n",
    "\n",
    "\n",
    "\n",
    "#df.show(truncate=False)\n",
    " # Cột dùng để partition\n",
    "#df_silver.show(truncate=False)\n",
    "\n",
    "duck_path = '/mnt/d/learn/DE/Semina_project/datawarehouse.duckdb'\n",
    "con = duckdb.connect(duck_path)\n",
    "# read file sql to connect to minio and create dim fact table\n",
    "\n",
    "# đoạn code dưới được đưa vào khi select from read_parquet(path), khi đọc file parquet trực tiếp vào duckdb thì\n",
    "# run code nó sẽ set up các config cho minio để kết nối trực tiếp đến duckdb -> duckdb có thể read direct data in minio\n",
    "# nếu ko có ko cần thêm vào\n",
    "'''with open('/mnt/d/learn/DE/Semina_project/SQL_db/config_dw/warehouse_source.sql', 'r') as f:\n",
    "     sql_script = f.read()\n",
    "con.execute(sql_script)'''\n",
    "\n",
    "# dki dataframe thành bảng ảo arrow_table_virtual để query sql\n",
    "# convert pyspark dataframe to arrow table\n",
    "arrow_table = pa.Table.from_pandas(df_silver.toPandas())\n",
    "con.register(\"arrow_table_virtual\", arrow_table)\n",
    "con.execute('''\n",
    "            INSERT INTO fact_trades(symbol,\n",
    "                                    tradeId,\n",
    "                                    side,\n",
    "                                    price,\n",
    "                                    quantity,\n",
    "                                    trade_time,\n",
    "                                    ingestion_time)\n",
    "            SELECT symbol,\n",
    "                   tradeId,\n",
    "                   side,\n",
    "                   price,\n",
    "                   quantity,\n",
    "                   trade_time,\n",
    "                   ingestion_time\n",
    "            FROM arrow_table_virtual\n",
    "            ''')\n",
    "#format data and load into minio with parquet\n",
    "output = f\"s3a://trading-okx/silver/okx_trades/\"\n",
    "df_silver.write.mode('append').format(\"parquet\").save(output)\n"
   ],
   "id": "484e8e3cce8cce80",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T16:10:15.897573Z",
     "start_time": "2025-12-03T16:10:15.596614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "con.close()\n",
    "spark.stop()\n"
   ],
   "id": "76654cc5b25c618c",
   "outputs": [],
   "execution_count": 23
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
